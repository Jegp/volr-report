\documentclass[a4paper,oneside]{memoir}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{wallpaper}
\usepackage{palatino}
\usepackage{hyperref}
\usepackage{csquotes}
\usepackage{listings}
%linespacing
\usepackage{setspace}
\renewcommand{\baselinestretch}{1.5}

% Bibliography
\usepackage[style=authoryear]{biblatex}
\addbibresource{bib.bib}
\bibliography{bib}

% Promote sections and subsections
\setheadfoot{\onelineskip}{2\onelineskip}
\setheaderspaces{*}{1mm}{*}
% \chapterstyle{plain} % needed?
\checkandfixthelayout

\renewcommand{\thesection}{\arabic{section}}
\makeatletter
\let\l@section\l@chapter
\makeatother

\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
\makeatletter
\let\l@subsection\l@section
\let\l@section\l@chapter
\makeatother

% Glossary
\usepackage[numberedsection=nameref]{glossaries}
\renewcommand{\glossarypreamble}{\label{glos}}
\makeglossaries
\include{glossary}
\makeglossaries
% Setup captions
\captionstyle[\centering]{\centering}
\changecaptionwidth
\captionwidth{0.8\linewidth}

% Protect against widows and orphans
%\clubpenalty=10000
%\widowpenalty=10000

%\linespread{1.2}

\raggedbottom

\chapterstyle{ger}

\maxsecnumdepth{subsection}

%%  Setup fancy style quotation
%%  ==================================================================
%\usepackage{tikz}
%\usepackage{framed}

%\newcommand*\quotefont{\fontfamily{fxl}} % selects Libertine for quote font

% Make commands for the quotes
%\newcommand*{\openquote}{\tikz[remember picture,overlay,xshift=-15pt,yshift=-10pt]
%     \node (OQ) {\quotefont\fontsize{60}{60}\selectfont``};\kern0pt}
%\newcommand*{\closequote}{\tikz[remember picture,overlay,xshift=15pt,yshift=5pt]
%     \node (CQ) {\quotefont\fontsize{60}{60}\selectfont''};}

% select a colour for the shading
%\definecolor{shadecolor}{rgb}{1,1,1}

% wrap everything in its own environment
%\newenvironment{shadequote}%
%{\begin{snugshade}\begin{quote}\openquote}
%{\hfill\closequote\end{quote}\end{snugshade}}

%%  Begin document
%%  ==================================================================
\begin{document}

%%  Begin title page
%%  ==================================================================
    \thispagestyle{empty}
    \ULCornerWallPaper{1}{ku-coverpage/nat-farve.pdf}
    \ULCornerWallPaper{1}{ku-coverpage/diku-en.pdf}
    \begin{adjustwidth}{-3cm}{-1.5cm}
    %\vspace*{-1cm}
    %\textbf{\Huge Free topic} \\
    \vspace*{2.5cm}
    \textbf{\Huge Modelling learning systems} \\
    \vspace*{.8cm}
    {\huge  A DSL for cognitive neuroscientists}\\
    \begin{tabbing}
    % adjust the hspace below for the longest author name
    Jens Egholm Pedersen \hspace{1cm} \= \texttt{<xtp778@alumni.ku.dk>} \\
    \\[11cm]

    \textbf{\Large Supervisor} \\
    Martin Elsman \hspace{1cm} \texttt{<mael@di.ku.dk>}
    \end{tabbing}
    \end{adjustwidth}

    \newpage

    \ClearWallPaper
%%  ==================================================================
%%  End title page


\section{Introduction}
In the past years machine learning has surpassed humans in some recognition
tasks, and the development shows no signs of slowing down.
These developments are however based on relatively old research on neural
networks \autocite{Nilsson2009, russel2007}.
Newer investigation into rehabilitation and learning indicates that such
networks alone cannot account for the same amount of learning that happens
in the brain \autocite{Mogensen2011, block2007, russel2007, Moravec98, dennett2017}.
For that reason the breakthroughs in machine learning are hard to transfer
to the domain of cognitive neuroscience.

As an attempt to remedy this, this project sets out to define a domain-specific
language (DSL) that is capable of representing the concepts of learning systems
within the domain of neuroscience.
The latter part of the paper validates this DSL through the modelling of a small
learning task. The benchmark will be written in \gls{futhark} and compiled to
the \gls{opencl} standard, but the DSL abstraction allows it to be executed on
any machine architecture.

The goal is for the DSL to lay the foundation for a more accurate scientific
representation of learning and learning concepts, serving as a more approachable
simulation tool for cognitive neuroscience.

%\subsection{Structure}
%This paper is structered ...

\subsection{Problem statement}
Building on theories and concepts of the domain of cognitive neuroscience
this paper examines the hypothesis that
\textit{
  the DSL presented in this paper can model meaningful machine learning
  tasks for the cognitive neurosciences,
  agnostic of the learning system implementation}.
The paper will approach this in two steps:

\begin{enumerate}
  \item Defining and implementing a DSL abstraction for the expression of
        learning tasks, based on the REF model from \autocite{Mogensen2011}.
  \item Testing the DSL by expressing a learning task in a Krechevsky
        T-maze \autocite{Krechevsky1932}, backed by a traditional machine
        learning implementation in \gls{futhark}.
\end{enumerate}

\section{Theory}
This section accounts for the theoretical foundation of paper and is divided
into three parts.
The first part concerns the broad topic of computation and learning in neural
systems as seen from the perspective of computational neuroscience. By focusing
on cognition, plasticity, learning and rehabilitation, it derives
the necessary and sufficient language abstractions to capture the complexity
of the domain.
The second part introduces traditional machine learning from the perspective of
computer science. These concepts will be applied in the validation phase of
learning model abstractions in section \ref{case}.
In the final part the theoretical background for linguistic abstractions and
the construction of domain specific languages will be treated.

\subsection{Computation and learning in neural systems}
\begin{quote}
  Activity-dependent synaptic plasticity is widely believed to be the basic
  phenomenon underlying learning and memory \autocite{dayan2001}.
\end{quote}

Commonly referred to as \textit{what fires together, wires together}, Hebbian
learning suggests that synaptic connections from neuron $A$ to neuron $B$
are strengthened or weakened when neuron $A$ excites or inhibits the chance of
firing neuron $B$ respectively \autocite{dayan2001}.
Hebbian learning is believed to play a large part in the plastic nature of the
brain, especially within learning and memory formation
\autocite{dayan2001, Johnston2009, Robertson1999}.

\subsubsection{Reorganisation of elementary functions}
\label{ref}

\autocite{Robertson1999} studied patients during
rehabilitation of brain damage and conjectured that learning --- whether when the
brain acquires new information or recovers from lost information --- occurs based
on the structural changes induced by the Hebbian principle
\autocite{Robertson1999}. They further concluded that the damaged brain areas
regenerate themselves based on this principle \autocite{Robertson1999}.
\autocite{Mogensen2011} refutes this point by claiming that, while there
may be some synaptogenesis, the rehabilitation mainly occurs when other parts
of the brain learns to take over the lost functions \autocite{Mogensen2011}.

\autocite{Mogensen2011} arrives at a theoretical framework which divides the
brain into localized and highly specialised, basic information
processing elementary functions (EF). These modular functions are contained
in a \textit{substructure} or \textit{local circuit}
within the brain \autocite{Mogensen2011}.

Multiple EFs interact to form algorithmic strategies (AS), established as
a consequence of learning and experiencing \autocite{Mogensen2011}. An
algorithmic strategy combines the capacity of multiple EFs into a single - and
for the AS appropriate - response \autocite{Mogensen2011, Mogensen2012b}.

The EF and AS interplay to create what Mogensen dubbed '\textit{surface
phenomena}', which manifests the behaviour of the system \autocite{Mogensen2011}.
Surface phenomena are the product of applying an AS to a particular problem,
and Mogensen hypothesised that a given AS is evaluated for every success or
failure of the surface behaviour predicted by that AS \autocite{Mogensen2011}.
Such evaluation either strengthens the AS's association with the given
behaviour scenario, or weakens et, potentially starting a search for another
AS to perform the task instead \autocite{Mogensen2011}.
According to Mogensen these changes are controlled by a specialised AS dubbed
the \textit{Supervisory Attentional System} (SAS) from \autocite{Norman1986},
manifested through neuroplastic changes in the synaptic connections within
the SAS \autocite{Mogensen2011}.

\textit{Behaviour} in the REF model is thus defined as the response of a single
AS (that, in turn, consists of a number of EFs) to a given task
\autocite{Mogensen2011, Mogensen2012b}.

%In an addition to the REF model, Mogensen introduced the algorithmic
%module (AM), which is a module similar to an AS, but whose computations can be
%shared by many AS \autocite{Mogensen2012b}. In other words they may not
%directly mediate "a task solution", but partake in many different task
%situations \autocite{Mogensen2017}.

An elaboration to the REF model arrived in the form of the REFGEN
model (general reorganisation of elementary functions) \autocite{Mogensen2017}.
The REFGEN model further explains the feedback mechanisms of the SAS
to account for the 'learning' or adaptive feature of neural systems, by
introducing two new concepts: the goal algorithmic strategy (GAS) and the
comparator \autocite{Mogensen2017, Mogensen2012b}.

In this new framework the SAS is tasked with maintaining the current state
of the system, while the GAS reflects the goal towards which it is desired to
to move (for instance the exit condition in a maze) \autocite{Mogensen2017}.
For this to be useful a comparator is needed to constantly compare the SAS
and GAS (the current state versus the goal), such as to select the optimal AS
for the task at hand \autocite{Mogensen2017}. The feedback (backpropagation)
from the actuation on the surrounding world is received by the comparator,
who will assert influence on the SAS and GAS to better account for the new
reality \autocite{Mogensen2017}.

\subsection{Machine learning}


\subsection{Language abstractions}

\section{Volr: A DSL for learning systems}
\label{volr}

\section{Case study: applying Volr in a Krechevsky maze}
\label{case}

\clearpage

\printglossary

\printbibliography

\section{Appendix A: Volr EBNF}
\label{ebnf}

\begin{verbatim}
  model = ( stimulus | strategy | response )
        , { [ space ] , "," , [ space ] , ( stimulus | strategy | response ) };

  stimulus = "stimulus" , [ space ] , name , [ space ] , dimensionality;
  response = "response" , [ space ] , dimensionality , [ space ] , input
           , [ space ] , { "select" , [ space ] , ( "random" | "best" ) };
  strategy = "strategy" , [ space ] , name , [ space ] , input
           , [ space ] , ( functions | function , [ space ] , { function } );

  functions = "functions" , [ space ] , dimensionality;
  function = "function" , [ space ] , name , dimensionality;

  input = "from" , [ space ] , name , { [ space ] , name };
  dimensionality = "[" , [ space ] , integer , [ space ] , "]";

  name list = name , { [ space ] , "," , name };
  name = letter , { letter | digit };
  letter = ? non-whitespace Unicode character ?;
  space = space character , { space character };
  space character = ? white space character ?;

  number = integer , ["." , digit , {digit}];
  integer = ["-"] , digit , {digit};
  digit = "0" | "1" | "2" | "3" | "4"
        | "5" | "6" | "7" | "8" | "9";
\end{verbatim}

\section{Appendix B: Learning network with two features in Volr}

\lstinputlisting{two_features.volr}

\end{document}
