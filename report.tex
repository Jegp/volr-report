\documentclass[a4paper,oneside]{memoir}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{wallpaper}
\usepackage{palatino}
\usepackage{hyperref}
\usepackage{csquotes}
\usepackage{listings}
%linespacing
\usepackage{setspace}
\renewcommand{\baselinestretch}{1.5}

% Bibliography
\usepackage[style=authoryear]{biblatex}
\addbibresource{bib.bib}
\bibliography{bib}

% Promote sections and subsections
\setheadfoot{\onelineskip}{2\onelineskip}
\setheaderspaces{*}{1mm}{*}
% \chapterstyle{plain} % needed?
\checkandfixthelayout

% \renewcommand{\thesection}{\arabic{section}}
% \makeatletter
% \let\l@section\l@chapter
% \makeatother
% %
% % \renewcommand{\thesection}{\arabic{section}}
% % \renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
% % \makeatletter
% % \let\l@subsection\l@section
% % \let\l@section\l@chapter
% % \makeatother

% Glossary
\usepackage[numberedsection=nameref]{glossaries}
\renewcommand{\glossarypreamble}{\label{glos}}
\makeglossaries
\include{glossary}
\makeglossaries
% Setup captions
\captionstyle[\centering]{\centering}
\changecaptionwidth
\captionwidth{0.8\linewidth}

% Protect against widows and orphans
%\clubpenalty=10000
%\widowpenalty=10000

%\linespread{1.2}

\raggedbottom

\chapterstyle{ger}

\maxsecnumdepth{subsection}

% Change memoir chapter margins
\usepackage{titlesec}
\titleformat{\chapter}[display]
{\normalfont\huge\bfseries}{\chaptertitlename\ \thechapter}{5pt}{\Huge}
\titlespacing*{\chapter}{10pt}{10pt}{10pt}

%%  Setup fancy style quotation
%%  ==================================================================
%\usepackage{tikz}
%\usepackage{framed}

%\newcommand*\quotefont{\fontfamily{fxl}} % selects Libertine for quote font

% Make commands for the quotes
%\newcommand*{\openquote}{\tikz[remember picture,overlay,xshift=-15pt,yshift=-10pt]
%     \node (OQ) {\quotefont\fontsize{60}{60}\selectfont``};\kern0pt}
%\newcommand*{\closequote}{\tikz[remember picture,overlay,xshift=15pt,yshift=5pt]
%     \node (CQ) {\quotefont\fontsize{60}{60}\selectfont''};}

% select a colour for the shading
%\definecolor{shadecolor}{rgb}{1,1,1}

% wrap everything in its own environment
%\newenvironment{shadequote}%
%{\begin{snugshade}\begin{quote}\openquote}
%{\hfill\closequote\end{quote}\end{snugshade}}

%%  Begin document
%%  ==================================================================
\begin{document}

%%  Begin title page
%%  ==================================================================
    \thispagestyle{empty}
    \ULCornerWallPaper{1}{ku-coverpage/nat-farve.pdf}
    \ULCornerWallPaper{1}{ku-coverpage/diku-en.pdf}
    \begin{adjustwidth}{-3cm}{-1.5cm}
    %\vspace*{-1cm}
    %\textbf{\Huge Free topic} \\
    \vspace*{2.5cm}
    \textbf{\Huge Modelling learning systems} \\
    \vspace*{.8cm}
    {\huge  A DSL for cognitive neuroscientists}\\
    \begin{tabbing}
    % adjust the hspace below for the longest author name
    Jens Egholm Pedersen \hspace{1cm} \= \texttt{<xtp778@alumni.ku.dk>} \\
    \\[11cm]

    \textbf{\Large Supervisor} \\
    Martin Elsman \hspace{1cm} \texttt{<mael@di.ku.dk>}
    \end{tabbing}
    \end{adjustwidth}

    \newpage

    \ClearWallPaper
%%  ==================================================================
%%  End title page

\renewcommand\cftchapteraftersnumb{\normalfont}
\renewcommand\cftbeforechapterskip{5pt plus 1pt}

\frontmatter
\tableofcontents*
\newpage

\mainmatter
\chapter{Introduction}
In the past years machine learning has surpassed humans in some recognition
tasks, and the development shows no signs of slowing down.
These developments are however based on relatively old research on neural
networks \autocite{Nilsson2009, russel2007}.
Newer investigation into rehabilitation and learning indicates that such
networks alone cannot account for the same amount of learning that happens
in the brain \autocite{Mogensen2011, block2007, russel2007, Moravec98, dennett2017}.
For that reason the breakthroughs in machine learning are hard to transfer
to the domain of cognitive neuroscience.

As an attempt to remedy this, this project sets out to define a domain-specific
language (DSL) that is capable of representing learning systems that resembles
those from the domain of neuroscience.

The latter part of the paper validates this DSL through the modelling of a small
learning task. The benchmark will be written in \gls{futhark} and compiled to
the \gls{opencl} standard. The language abstraction however, allows it to be
executed on any machine architecture.

The goal is for the DSL to explore a more accurate scientific representation of
learning and learning concepts, serving as a more approachable simulation tool
for the cognitive neurosciences.

\section{Problem statement}
Building on theories and concepts of the domain of cognitive neuroscience
this paper examines the hypothesis that
\textit{
  the DSL presented in this paper can model meaningful machine learning
  tasks for the cognitive neurosciences,
  agnostic of the learning system implementation}.
The paper will approach this in two steps:

\begin{enumerate}
  \item Defining and implementing a DSL abstraction for the expression of
        learning tasks, based on the REF model from \autocite{Mogensen2011}.
  \item Testing the DSL by expressing a learning task in a Krechevsky
        T-maze \autocite{Krechevsky1932}, backed by a traditional machine
        learning implementation in \gls{futhark}.
\end{enumerate}
%
% \section{Structure}
% This paper is built around theoretical concepts from cognitive neuroscience,
% \gls{ml} and finally

{\let\clearpage\relax\chapter{Theory}}
This section accounts for the theoretical foundation of paper and is divided
into three parts.
The first part concerns the broad topic of computation and learning in neural
systems as seen from the perspective of computational neuroscience. By focusing
on cognition, plasticity, learning and rehabilitation, it derives
the necessary and sufficient language abstractions to capture the complexity
of the domain.
The second part introduces traditional machine learning from the perspective of
computer science. These concepts will be applied in the validation phase of
learning model abstractions in section \ref{case}.
In the final part the theoretical and methodological background for the
development of \gls{dsl}s will be treated.

\section{Learning in neural systems}
\begin{quote}
  Activity-dependent synaptic plasticity is widely believed to be the basic
  phenomenon underlying learning and memory \autocite{dayan2001}.
\end{quote}

Commonly referred to as \textit{what fires together, wires together}, Hebbian
learning suggests that synaptic connections from neuron $A$ to neuron $B$
are strengthened or weakened when neuron $A$ excites or inhibits the chance of
firing neuron $B$ respectively \autocite{dayan2001}.
Hebbian learning is believed to play a large part in the plastic nature of the
brain, especially within learning and memory formation
\autocite{dayan2001, Johnston2009, Robertson1999}.

\subsection{Reorganisation of elementary functions}
\label{ref}

\autocite{Robertson1999} studied patients during
rehabilitation of brain damage and conjectured that learning --- whether when the
brain acquires new information or recovers from lost information --- occurs based
on the structural changes induced by the Hebbian principle
\autocite{Robertson1999}. They further concluded that the damaged brain areas
regenerate themselves based on this principle \autocite{Robertson1999}.
\autocite{Mogensen2011} refutes this point by claiming that, while there
may be some synaptogenesis, the rehabilitation mainly occurs when other parts
of the brain learns to take over the lost functions \autocite{Mogensen2011}.

\autocite{Mogensen2011} arrives at a theoretical framework which divides the
brain into localized and highly specialised, basic information
processing elementary functions (EF). These modular functions are contained
in a \textit{substructure} or \textit{local circuit}
within the brain \autocite{Mogensen2011}.

Multiple EFs interact to form algorithmic strategies (AS), established as
a consequence of learning and experiencing \autocite{Mogensen2011}. An
algorithmic strategy combines the capacity of multiple EFs into a single - and
for the AS appropriate - response \autocite{Mogensen2011, Mogensen2012b}.

The EF and AS interplay to create what Mogensen dubbed '\textit{surface
phenomena}', which manifests the behaviour of the system \autocite{Mogensen2011}.
Surface phenomena are the product of applying an AS to a particular problem,
and Mogensen hypothesised that a given AS is evaluated for every success or
failure of the surface behaviour predicted by that AS \autocite{Mogensen2011}.
Such evaluation either strengthens the AS's association with the given
behaviour scenario, or weakens et, potentially starting a search for another
AS to perform the task instead \autocite{Mogensen2011}.
According to Mogensen these changes are controlled by a specialised AS dubbed
the \textit{Supervisory Attentional System} (SAS) from \autocite{Norman1986},
manifested through neuroplastic changes in the synaptic connections within
the SAS \autocite{Mogensen2011}.

\textit{Behaviour} in the REF model is thus defined as the response of a single
AS (that, in turn, consists of a number of EFs) to a given task
\autocite{Mogensen2011, Mogensen2012b}.

%In an addition to the REF model, Mogensen introduced the algorithmic
%module (AM), which is a module similar to an AS, but whose computations can be
%shared by many AS \autocite{Mogensen2012b}. In other words they may not
%directly mediate "a task solution", but partake in many different task
%situations \autocite{Mogensen2017}.

An elaboration to the REF model arrived in the form of the REFGEN
model (general reorganisation of elementary functions) \autocite{Mogensen2017}.
The REFGEN model further explains the feedback mechanisms of the SAS
to account for the 'learning' or adaptive feature of neural systems, by
introducing two new concepts: the goal algorithmic strategy (GAS) and the
comparator \autocite{Mogensen2017, Mogensen2012b}.

In this new framework the SAS is tasked with maintaining the current state
of the system, while the GAS reflects the goal towards which it is desired to
to move (for instance the exit condition in a maze) \autocite{Mogensen2017}.
For this to be useful a comparator is needed to constantly compare the SAS
and GAS (the current state versus the goal), such as to select the optimal AS
for the task at hand \autocite{Mogensen2017}. The feedback (backpropagation)
from the actuation on the surrounding world is received by the comparator,
who will assert influence on the SAS and GAS to better account for the new
reality \autocite{Mogensen2017}.

% TODO: Write about the reality -> encoding -> brain -> decoding -> reality loop

\section{Learning in machines}
Systems that display the capability of learning (or progressively improving on
a certain task) are historically associated with the field of \gls{ml}.
\Gls{ml} goes back to the 1950s, and is tightly coupled
with artificial intelligence --- the idea of creating intelligence
in machine \autocite{Nilsson2009, russel2007}.
It is a large and active research field, why this section will strictly
focus on a brief introduction and motivation of the topic in the context of
learning tasks. This will be followed by a more in-depth description of
multilayered perceptron networks and progressive learning through stochastic
gradient descent. Both techniques will be employed in the implementation
and validation sections (sections \ref{volr} and \ref{case}).

\subsection{Artificial intelligence and machine learning}
Nilsson defines \gls{ai} as the "activity devoted to making machines
intelligent", where \textit{intelligence} is defined as the "quality that
enables an entity to function appropriately and with foresight in its
environment" \autocite[13]{Nilsson2009}. As Nilsson also points out, this
covers a large range of systems such as recommender systems and image recognition
tasks, which does not display higher cognitive functions \autocite[13]{Nilsson2009}.
In the context of this paper, the above definition suffices to cover basic
learning progression.

\subsection{Learning in perceptron networks}
Inspired by the biological brain, models for neural networks have been applied
in computer science since the 1950s, and have become integral part of machine
learning \autocite{Nilsson2009, russel2007}.
Neurons are essentially functions that operate on some input and respond with
some output \autocite{russel2007}. The inputs for a neuron are weighted to give
different input channels - or input dimensions - varying significance. These
weighted inputs arrive to an activation function that decides whether the
neuron should ‘fire’ or not \autocite{Nilsson2009}. Sigmoid or hyperbolic tangent
are popular
activation functions for numerical predictions because of their steep logistic
properties while retaining differentiability. Neural networks excel in their
adaptability and have been applied to many domains with great success
\autocite{schmidhuber2014, russel2007, Pedersen2017}.

Simple neural networks can be stacked in layers, where each neuron will assign
weights to the input, to determine its significance for the activation function
\autocite{Nilsson2009}. By tuning the weights of the neurons, such groupings can
learn to fire on certain input patterns \autocite{russel2007}. However,
this structure have proved brittle and difficult to train because the
neurons do not retain their weights for long \autocite{Nilsson2009, russel2007}.
\autocite{Rumelhart1988} suggested a method to avoid this instability by adjusting
the weights of the neurons in retrospect with a method called back-propagation
\autocite{Rumelhart1988, Nilsson2009}. This optimisation is
operationalised by a function that can describe the \textit{loss} of efficiency
in a network, and then adjust the weights correspondingly using gradient
descent \autocite{russel2007}.

\section{Language abstractions}
A domain specific language (DSL) is a generalisation of a domain into a
collection of tokens \autocite{Mernik2005}. DSL generally offers more
expressiveness and require less programming expertise \autocite{Mernik2005, Sestoft2017}.
However, designing DSLs requires both deep domain knowledge and language
development expertise and bad DSL design can become limiting
and counterproductive \autocite{Mernik2005, Sestoft2017}.

Compared to the development of general programming languages, DSL development
is less standardised within the literature \autocite{Mernik2005, Deursen2002}.
However, there are strong recommendations to perform rigorous analyses of the
target domain and extract precise features necessary for the DSL construction
\autocite{Mernik2005, Deursen2002}.

DSLs can be implemented either as a standalone language or embedded in a more
general programming language \autocite{Mernik2005}. A standalone approach
leverages the freedom of an independent syntax, but avoids the benefits of
the environment and tools that normally follows a programming language
\autocite{Mernik2005, Sestoft2017}. Conversely, an embedded approach will be
more restricted with regards to syntax, but retains the infrastructure of the
host language \autocite{Mernik2005}.\footnote{\cite{Mernik2005} lists a
number of detailed benefits and downsides for either approaches. See
\autocite[330-331]{Mernik2005}.}

Futher, a DSL can either reuse the syntax and structure of other, older
languages or completely invent new constructions
\autocite{Mernik2005, Deursen2002}. DSLs without any resemblance to previous
languages and paradigms are difficult to develop and hard to memorise,
especially for the user \autocite{Wile2004}.

The target for the DSL can, but does not have to be, executable
\autocite{Mernik2005}. One common approach for executable DSLs
is to generate code that can be compiled by a separate environment, thus
promoting reuse of code \autocite{Wile2004}.

Lastly it is important to note that the introduction of a DSL is not only
introducing the language abstraction itself, but can impose up to five
innovations: artifacts, language, tools, infrastructure and methodology
\autocite{Wile2004}. For that reason it is important for the DSL to be as
close to the original domain as possible, without introducing unnecessary
overhead \autocite{Wile2004, Mernik2005}.

{\let\clearpage\relax\chapter{Volr: A DSL for learning systems}}
\label{volr}

Following the advise of \cite{Mernik2005} and \cite{Wile2004}, the theory of
reorganisation of elementary functions from section \ref{ref}, has been
applied to the domain of \gls{ml} as rigorously as possible.
The resulting DSL has been dubbed \texttt{Volr}.

Before accounting for the features and design decision of the language, it is
important to motivate the reasoning for constructing a DSL.
Research within the domain of (cognitive) neuroscience is juggling increasingly
complex models, and are to an increasing degree overlapping with the domain of
computer science.\footnote{This claim is implicit in a number of articles from both
sides, and explicit in more recent literature. From the perspective of computer
science, see for instance \autocite{Nilsson2009, walter2015, schmidhuber2014,
russel2007}. From the perspective of cognitive science, see \autocite{Hohwy2009,
dennett2017, dayan2001, sep:cognitive-science}.} Researchers within cognitive
sciences are rarely equipped with strong programming knowledge, and the rapid
development of \gls{ml} in the recent years has made it hard to keep track of
state-of-the-art frameworks.\footnote{Within \gls{ml} alone there have been
a surge of new projects in the last few years. Of particular note are
Google's Tensorflow (from 2015), Microsoft Cognitive Toolkit (from 2016),
Python's Scikit-learn (from 2007) and Pytorch (from 2016).}

Another fact driving the decision to design a DSL was the particular need to
build and extend specific features for cognitive neuroscience, such as
neuron clusters (without layering) and lesions in the networks. These features
would never enter regular \gls{ml} libraries, because of their strong
focus on accuracy and performance \autocite{Nilsson2009, schmidhuber2014}.

\section{Volr design decisions}
Because of the requirement to design a few, but unique features a standalone
approach was employed. The syntax has been kept simple and resembles that of
the YAML (YAML Ain't Markup Language, \cite{yaml}). This style was chosen for
its simplicity, widespread use along with its ability to express declarations.

The language is parsed in Haskell using the open-source parser-combinator
library \texttt{megaparsec} \autocite{mepaparsec}, and is available on GitHub
\autocite{Pedersen2018:volr}.

The DSL was clearly required to be executable to allow users to evaluate and
analyse their models. The only backend supporting this currently is implemented
in the data-parallel array programming language Futhark
\autocite{Henriksen2017}, running on an \gls{opencl} backend. The implementation
is also available on GitHub \autocite{Pedersen2018:futhark}.

\section{Volr concepts}


{\let\clearpage\relax\chapter{Case study: Krechevsky maze}}
\label{case}

As pointed out by \cite{Mernik2005} quantitative evaluation of a DSL is hard,
which is why this section will....

\chapter{Discussion}

\section{Conclusion}

\section{Future work}
% Error reporting
% A note on competition within the brain (feral neurons)

\clearpage

\printglossary

\printbibliography

\newpage

\appendix
\chapter{Volr EBNF}
\label{ebnf}

\begin{verbatim}
  model = ( stimulus | strategy | response )
        , { [ space ] , "," , [ space ] , ( stimulus | strategy | response ) };

  stimulus = "stimulus" , [ space ] , name , [ space ] , dimensionality
           , [ space ] , "file:" , [ space ] , name;
  strategy = "strategy" , [ space ] , name , [ space ] , input
           , [ space ] , "functions:" , [ space ] , integer;
  response = "response" , [ space ] , dimensionality , [ space ] , input
           , [ space ] , "file:" , [ space ] , name
           , [ space ] , "learning_rate:" , [ space ] , number;

  input = "from" , [ space ] , name , { [ space ] , name };
  dimensionality = "[" , [ space ] , integer , [ space ] , "]";

  name list = name , { [ space ] , "," , name };
  name = letter , { letter | digit };
  letter = ? non-whitespace Unicode character ?;
  space = space character , { space character };
  space character = ? white space character ?;

  number = integer , ["." , digit , {digit}];
  integer = ["-"] , digit , {digit};
  digit = "0" | "1" | "2" | "3" | "4"
        | "5" | "6" | "7" | "8" | "9";
\end{verbatim}

\clearpage

\appendix
\chapter{Learning network with two features in Volr}

\lstinputlisting{two_features.volr}

\clearpage

\end{document}
